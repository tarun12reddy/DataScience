---
title: "Movie Recommendation - KNN Kernel Model"
author: "Tarun Reddy Aleti"
date: "02/27/2016"
output: html_document
---

### Executive Summary
- The datasets required for the analysis can be downloaded from these links [Movies](https://inclass.kaggle.com/c/movie/download/movies.dat), [Users](https://inclass.kaggle.com/c/movie/download/users.dat), [Ratings](https://inclass.kaggle.com/c/movie/download/training_ratings_for_kaggle_comp.csv) and [ReadMe](https://inclass.kaggle.com/c/movie/download/README) 
- Idea is predict with movie recommendation ratings given by Users.
- I have used a K-Nearest Neighbour Approach wrapped with Kernel Method.
- Model accuracy can be evaluated by taking average of the predicted and real ratings for top 5% of highest ratings per User.

#### Required Packages

Following packages are required for the code to run efficiently.

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
library(MASS)
library(rjson)
library(plyr)
library(knitr)
library(caret)
library(fmsb)
library(e1071)
library(randomForest)
library(caretEnsemble)
library(ROCR)
library(ineq)
library(zipcode)
library(sqldf)
library(ggplot2)
```

### Preliminary Analysis

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
set.seed(1)
options(scipen = 999)
movies <- read.delim("movies.dat", header = FALSE, sep = ":", stringsAsFactors = FALSE)
movies <- movies[ ,seq(1, 5, 2)]
colnames(movies) <- c("MovieID", "Title", "Genres")
users <- read.delim("users.dat", header = FALSE, sep = ":", stringsAsFactors = FALSE)
users <- users[ ,seq(1, 9, 2)]
colnames(users) <- c("UserID", "Gender", "Age", "Occupation", "Zipcode")
ratings <- read.csv("training_ratings_for_kaggle_comp.csv", header = TRUE, stringsAsFactors = FALSE)
```

- We have three datasets in hand - Movies, Users and Ratings
  1. *Movies*: It has three fields MovieID, MovieTitle and Genres.
    + We can extract the year movie was released using the title name. We can further extract the decade from the year.
    + Movie can belong to more than one of the Genre. We need to create a dummy variable indicating if the movie belong to that Genre or not.
  2. *Users*: It has five fields UserID, Gender, Age, Occupation and Zipcode
    + We can use Zipcode information and extract the city, state the Users belong to.
    + Age is already categorized into 6 categories and Occupation into 20 categories as can be from Readme file.
  3. *Ratings*: It has four field UserID, MovieID, Rating and User_Movie_ID
    + All the fields are self explanatory.
    + We need to join all these tables and Rating is our dependent variable for our model.
    
```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
genres <- c("Action", "Adventure", "Animation", "Children's",
            "Comedy", "Crime", "Documentary", "Drama", "Fantasy",
            "Film-Noir", "Horror", "Musical", "Mystery", "Romance",
            "Sci-Fi", "Thriller", "War", "Western")

title_year <- adply(movies[ ,'Title'], 1 , function(x){
                                              t <- gsub("[a-zA-Z()/' ']", "", substr(x, nchar(x)-6, nchar(x)))
                                              if(nchar(t) == 4 & as.numeric(t) <= 2001){
                                                return(t)
                                              } else {
                                                return(NA)
                                              }
                                            })
title_year <- title_year[ ,-1]

movie_genre <- adply(movies[ ,'Genres'], 1, function(x){
                                          dummy_genre <- rep(0, length(genres))
                                          g <- strsplit(x, split = '[|]')
                                          index_g <- aaply(g[[1]], 1, function(x){
                                                                      return(which(genres == x))
                                                                   })
                                          dummy_genre[index_g] <- 1
                                          return(dummy_genre)
                                        })

movie_genre <- movie_genre[ ,-1]
colnames(movie_genre) <- paste("genres", genres, sep = "_")
movies <- cbind(movies[ ,-3], title_year)
movies <- cbind(movies, movie_genre)
movies$total_genres <- apply(movies, 1, function(x){return(sum(as.numeric(x[4:21])))})

data(zipcode)
users$cleanZipcode <- aaply(users$Zipcode, 1, function(x){return(substr(x, 1, 5))})

query <- "SELECT u.*, z.zip, z.city, z.state
          FROM users as u
          LEFT JOIN zipcode as z on u.cleanZipcode = z.zip"

user_zip <- sqldf(query)
```

For `r length(unique(user_zip[is.na(user_zip$zip), 'Zipcode']))` zips I wasn't able to identify the city and state using the free zipcode dataset available in R. Need google API's to locate the city and state for them because these weren't invalid zips. 
Zips we couldn't identify city and state are `r unique(user_zip[is.na(user_zip$zip), 'Zipcode'])`

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
query <- "SELECT m.*, u.*, z.zip, z.city, z.state, r.rating
          FROM ratings as r
          INNER JOIN movies as m on r.movie = m.MovieID
          INNER JOIN users as u on r.user = u.UserID
          LEFT JOIN zipcode as z on u.cleanZipcode = z.zip"

data <- sqldf(query)

data$decade <- as.character(1900 + 10*floor((as.numeric(data$title_year) - 1900)/10))
data[is.na(data$title_year), 'title_year'] <- 'NA'
data[is.na(data$Gender), 'Gender'] <- 'NA'
data$Age <- as.character(data$Age)
data$Occupation <- as.character(data$Occupation)
data[is.na(data$Age), 'Age'] <- 'NA'
data[is.na(data$Occupation), 'Occupation'] <- 'NA'
data[is.na(data$state), 'state'] <- 'NA'
data[is.na(data$decade), 'decade'] <- 'NA'
data$UserID <- as.character(data$UserID)
```

### Exploratory data analysis

Following plots can help in analyzing effect of the independent variable on the dependent variable

1. *Frequency plot for scoring*

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
ggplot(data, aes(rating)) + geom_bar() + ggtitle('Frequency of Ratings')
```

2. *Boxplot comparing Ratings for Movies and Users*

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
user_box <- ddply(data, ~UserID, summarise, mean = mean(rating))
movie_box <- ddply(data, ~MovieID, summarise, mean = mean(rating))

boxplot(user_box$mean, movie_box$mean, names = c("User", "Movie"),
        main = "Average Ratings per User and Movie",
        ylab = 'Average Rating')
```

3. *Frequency of Age group*

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
all_user <- sqldf("SELECT UserID, Age, Gender, Occupation, state
                   FROM data
                   GROUP BY UserID, Age, Gender, Occupation, state")

ggplot(all_user, aes(factor(Age))) + geom_bar() + ggtitle('Frequency of Age Group') + xlab('Age Group')
```

4. *Barplot illustrating average rating across Age groups*

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
ggplot(data, aes(x=factor(Age), y=rating)) + stat_summary(fun.y="mean", geom="bar") +
  ggtitle("Average Rating across Age Group") + xlab('Age Group') + ylab('Average Rating')
```

5. *Frequency of Sex*

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
ggplot(all_user, aes(factor(Gender))) + geom_bar() + ggtitle('Frequency of Gender') + xlab('Gender')
```

6. *Barplot illustrating average rating across Gender*

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
ggplot(data, aes(x=factor(Gender), y=rating)) + stat_summary(fun.y="mean", geom="bar") +
  ggtitle("Average Rating across Gender") + xlab('Gender') + ylab('Average Rating')
```

7. *Frequency of Occupation*

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
ggplot(all_user, aes(factor(Occupation))) + geom_bar() + ggtitle('Frequency of Occupation') + xlab('Occupation')
```

8. *Barplot illustrating average rating across Occupation*

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
ggplot(data, aes(x=factor(Occupation), y=rating)) + stat_summary(fun.y="mean", geom="bar") +
  ggtitle("Average Rating across Occupation") + xlab('Occupation') + ylab('Average Rating')
```

9. *Frequency of State*

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide', fig.width = 12, fig.height = 6}
ggplot(all_user, aes(factor(state))) + geom_bar() + ggtitle('Frequency of State') + xlab('State')
```

10. *Barplot illustrating average rating across State*

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide', fig.width = 12, fig.height = 6}
ggplot(data, aes(x=factor(state), y=rating)) + stat_summary(fun.y="mean", geom="bar") +
  ggtitle("Average Rating across State") + xlab('State') + ylab('Average Rating')
```

11. *Frequency of Decade*

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
movie_decade <- sqldf("SELECT MovieID, decade
                       FROM data
                       GROUP BY MovieID, decade")
ggplot(movie_decade, aes(factor(decade))) + geom_bar() + ggtitle('Frequency of Movies per Decade') + xlab('Decade Year')
```

12. *Barplot illustrating average rating across Decade*

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
ggplot(data, aes(x=factor(decade), y=rating)) + stat_summary(fun.y="mean", geom="bar") +
  ggtitle("Average Rating across Decade") + xlab('Decade Year') + ylab('Average Rating')
```

13. *Number of Genre movie belong to*

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
ggplot(movies, aes(factor(total_genres))) + geom_bar() + ggtitle('Frequency of Total Genres') + xlab('Total Genres')
```

14. *Barplot illustrating average rating across total genres*

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
ggplot(data, aes(x=factor(total_genres), y=rating)) + stat_summary(fun.y="mean", geom="bar") +
  ggtitle("Average Rating across Total Genres") + xlab('Total Genres') + ylab('Average Rating')
```

From all the plots we can see that average rating across the fields is almost same.
Only the boxplot figure shows that average rating across Users or per Movie has lot of variability

### Train and Test Samples

- Due to limitation of my computer configuration I'm limiting the data to only 100 Users. Will be choosing these Users randomly. 
- In the original data set there are `r dim(users)[1]` unique Users and `r dim(movies)[1]` unique Movies.
- We create 70-30 parition on the selected data for train and test samples respectively

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
slctd_users <- sample(unique(data$UserID), 100)
data <- data[ ,c('MovieID', 'UserID', setdiff(colnames(data), c('title_year', 'Zipcode', 'cleanZipcode', 
                                         'cleanZipcode1', 'zip', 'city', 
                                         'total_genres', 'MovieID', 'UserID')))]
data <- data[!is.na(match(data$UserID, slctd_users)), ]
trainIndex <- createDataPartition(data$rating, p = .7, list = FALSE)
train <- data[trainIndex, ]
test <- data[-trainIndex, ]
```

- Size of train data set is **`r nrow(train)`** and test data set is **`r nrow(test)`** and there are **`r ncol(train)-1`** independent variables.

### KNN Modeling

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
knn <- function(training, k_nn, testing, wts, response){
  wt <- c(wts, rep(1, 24))
  train_res <- training[,response]  #get response variable from training data
  test_res <- testing[,response]    
  train_pred <- training[,-which(names(training) %in% response)]
  test_pred <- testing[,-which(names(testing) %in% response)]
  
  train_row_count <- nrow(train_pred)
  train_col_count <- ncol(train_pred)
  
  num_var <- sapply(train_pred,is.numeric)
  nom_var <- sapply(train_pred,is.character)

  train_pred_nom <- as.matrix(train_pred[,nom_var])
  train_pred_num <- as.matrix(train_pred[,num_var])
  
  test_pred[nom_var] <- lapply(test_pred[nom_var],as.character)
  
  dist <- apply(test_pred,1,
                  function(tx){
                    dist_num <- abs(sweep(train_pred_num,2,as.numeric(tx[num_var]),"-"))%*%(wt[num_var])
                    dist_nom <- abs(sweep(train_pred_nom,2,tx[nom_var],"!=")+0)%*%(wt[nom_var])    		
                    dist_all <- (dist_num+dist_nom)/sum(wt)
                    return(dist_all)
                  })

  shrtlstd_dist <- apply(dist, 2,
                         function(tx){
                           temp <- tx[order(tx)[1:k_nn]]
                           return(temp)
                         })
  
  pred <- apply(dist, 2, 
                function(tx){
                  temp <- train_res[order(tx)[1:k_nn]]
                  return(temp)
                })

  pred <- t(pred)
  shrtlstd_dist <- t(shrtlstd_dist)
  colnames(pred) <- paste("knn", 1:k_nn, sep = "_")
  colnames(shrtlstd_dist) <- paste("knn", 1:k_nn, sep = "_")
  return(list(predictions = pred, distances = shrtlstd_dist))
}
```

### Kernel Model

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
kernel <- function(pred, dist, type, k_nn){ 
  pred <- data.frame(pred[ ,1:k_nn])
  dist <- data.frame(dist[ ,1:k_nn])
  colnames(pred) <- paste("knn", 1:k_nn, sep = "_")
  colnames(dist) <- paste("knn", 1:k_nn, sep = "_")
  if(type == "Uniform"){
    wt <- data.frame(matrix(1, nrow = dim(pred)[1], ncol = k_nn))
  } 
  if(type == "Epanechsilov"){
    wt <- apply(dist, 2, function(tx){
                          return((3*(1-(tx)^2))/4)
                         })
  }
  if(type == "Guassian"){
    wt <- apply(dist, 2, function(tx){
                          return(exp(-(tx)^2)/sqrt(2*pi))
                         })
  }
  data <- cbind(pred, wt)
  final_pred <- apply(data, 1, 
                      function(tx){
                        return(sum((tx[1:k_nn]*tx[(k_nn+1):(2*k_nn)])/sum(tx[(k_nn+1):(2*k_nn)])))
                      })
  return(final_pred)
}
```


### Error Analysis

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
error <- function(test_data, pred){
  data <- cbind(test_data, data.frame(pred))
  data$pred <- -data$pred
  data <- data[order(data$UserID),]
  data$seq <- unlist(with(data, tapply(pred, UserID, function(x) rank(x, ties.method= "first"))))
  
  data$pred <- -data$pred
  query <- "SELECT d.UserID,
                   d.rating,
                   d.pred,
                   d.seq,
                   g.cnt,
                   g.maxseq
            FROM data as d
            INNER JOIN (SELECT UserID, count(*) as cnt, max(seq) as maxseq
                        FROM data
                        GROUP BY UserID) as g ON g.UserID = d.UserID
            ORDER BY d.UserID, d.pred desc"
  
  data$req_seq <- floor(data$seq*0.5) + 1
  data$rnd_pred <- round(data$pred)
  data <- data[data$req_seq == 1, ]
  return(list(kaggle_score = mean(c(data$rating, data$rnd_pred)), model_error = mean(abs(data$rating - data$rnd_pred))))
}
```

### Model

- Will be attempting a weighted K-Nearest Neighbours Approach. From exploratory analysis we see that average rating acorss independent variables is almost same except for MovieID and UserID. They have lot of variability. So will be trying different weights for them
- Will be performing analysis upto 50 nearest neighbours
- Will be using the kernel functions on the nearest distances to reevaluate the prediction
- Calculating mean absolute error for top 5% prediction rating across Users and also kaggle score as mentioned in the ReadMe file

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide', fig.width = 12, fig.height = 12}
final_result <- data.frame(matrix(0, nrow = (3*3*50*3), ncol = 6))
colnames(final_result) <- c("knn", "kernel", "movie_wt", "user_wt", "kaggle_score", "model_error")

cnt <- 1
for(movie_wt in c(1, 2, 3)){
  for(user_wt in c(1, 2, 3)){
    knn_result <- knn(training = train, k_nn = 50, testing = test, wts = c(movie_wt, user_wt), response = 'rating')
    for(knn_val in 1:50){
      for(krnl in c("Uniform", "Guassian", "Epanechsilov")){
        kernel_result <- kernel(pred = knn_result$predictions, dist = knn_result$distances, type = krnl, k_nn = knn_val)
        result <- error(test_data = test, pred = kernel_result)
        final_result[cnt, 'knn'] <- knn_val
        final_result[cnt, 'kernel'] <- krnl
        final_result[cnt, 'movie_wt'] <- movie_wt
        final_result[cnt, 'user_wt'] <- user_wt
        final_result[cnt, 'kaggle_score'] <- result$kaggle_score
        final_result[cnt, 'model_error'] <- result$model_error
        cnt <- cnt + 1
      }
    }
  }
}

final_result$movie_user_wt <- paste("movie", final_result$movie_wt, "user", final_result$user_wt, sep = "_")

ggplot(final_result, aes(x = knn, y = kaggle_score, colour = kernel, group = movie_wt)) + facet_wrap(~movie_user_wt) + geom_point() + 
xlab('K Nearest Neighbours') + ylab('Kaggle Score') + 
ggtitle('Kaggle Score for Various combinations of User and Movie Weights')

ggplot(final_result, aes(x = knn, y = model_error, colour = kernel, group = movie_wt)) + facet_wrap(~movie_user_wt) + geom_point() + 
xlab('K Nearest Neighbours') + ylab('Absolute Mean Error') + 
ggtitle('Mean Absolute Error for Various combinations of User and Movie Weights')
```

- Mean Absolute Error has a sinusoidal trend across various weight variations. It is high initially then decrease, increase like a sine curve.
- Kaggle score has decreasing trend and stabilizing at the end.
- Minimum Absolute Error for the model is `r min(final_result$model_error)` and it occurs in these scenarios 
```{r, echo = FALSE, warning = FALSE, message = FALSE} 
result <- final_result[final_result$model_error == min(final_result$model_error), c('knn', 'kernel', 'movie_wt', 'user_wt', 'kaggle_score')]
result
```
- Maximum Absolute Error for the model is `r max(final_result$model_error)` and it occurs in these scenarios 
```{r, echo = FALSE, warning = FALSE, message = FALSE}
final_result[final_result$model_error == max(final_result$model_error), c('knn', 'kernel', 'movie_wt', 'user_wt', 'kaggle_score')]
```
- Minimum Kaggle Score for the model is `r min(final_result$kaggle_score)` and it occurs in these scenarios 
```{r, echo = FALSE, warning = FALSE, message = FALSE}
final_result[final_result$kaggle_score == min(final_result$kaggle_score), c('knn', 'kernel', 'movie_wt', 'user_wt', 'model_error')]
```
- Maximum Kaggle Score for the model is `r max(final_result$kaggle_score)` and it occurs in these scenarios 
```{r, echo = FALSE, warning = FALSE, message = FALSE}
final_result[final_result$kaggle_score == max(final_result$kaggle_score), c('knn', 'kernel', 'movie_wt', 'user_wt', 'model_error')]
```
- Summary of Mean Absolute Error and Kaggle Score is as follows 
```{r, echo = FALSE, warning = FALSE, message = FALSE}
summary(final_result[ ,c('model_error', 'kaggle_score')])
```

### Conclusions
- Final model choosen is `r result[1, 'knn']`-Nearest Neighbours with `r result[1, 'kernel']` Kernel function and Movie.wt `r result[1, 'movie_wt']` times and User.wt `r result[1, 'user_wt']` times than other weights of independent variables.
- Kaggle score for our test model is `r result[1, 'kaggle_score']`
- The data sets in analysis are part of Kaggle Class problem Movie Recommendation
