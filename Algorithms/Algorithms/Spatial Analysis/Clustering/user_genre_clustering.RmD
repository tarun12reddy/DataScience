---
title: "Clustering"
author: "Tarun Reddy Aleti"
date: "April 4, 2016"
output: html_document
---
### Executive Summary
- The datasets required for the analysis can be downloaded from these links [Movies](https://inclass.kaggle.com/c/movie/download/movies.dat), [Ratings](https://inclass.kaggle.com/c/movie/download/training_ratings_for_kaggle_comp.csv) and [ReadMe](https://inclass.kaggle.com/c/movie/download/README) 
- Idea is segment users using cluster analysis techniques
- We use *Hierarchical Clustering* and *K-Means Clustering* methods for our analysis.

### Required Packages

Following packages are required for the code to run efficiently.

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
library(plyr)
library(sqldf)
library(psych)
library(NbClust)
```

### Preliminary Analysis

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
ratings <- read.csv("training_ratings_for_kaggle_comp.csv", header = TRUE, stringsAsFactors = FALSE)
movieLines <- readLines("movies.dat")
movies <- adply(movieLines, 1, function(x){
  y <- strsplit(x, split = ":")
  return(c(y[[1]][1], y[[1]][3], y[[1]][5]))
})
movies <- movies[ ,-1]
colnames(movies) <- c("MovieID", "Title", "Genres")

genres <- c("Action", "Adventure", "Animation", "Children's",
            "Comedy", "Crime", "Documentary", "Drama", "Fantasy",
            "Film-Noir", "Horror", "Musical", "Mystery", "Romance",
            "Sci-Fi", "Thriller", "War", "Western")

```

- We have two datasets in hand - Movies and Ratings
  1. *Movies*: It has three fields MovieID, MovieTitle and Genres.
    + We can extract the year movie was released using the title name. We can further extract the decade from the year.
    + Movie can belong to more than one of the Genre. We need to create a dummy variable indicating if the movie belong to that Genre or not.
  2. *Ratings*: It has four field UserID, MovieID, Rating and User_Movie_ID
    + All the fields are self explanatory.
    + We need to join all these tables and Rating is our dependent variable for our model.
    
### Feature Extraction

- Create dummy variables for each genre
- Calculate the average rating given by User for genres based upon ratings given by him for that genre

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
movie_genre <- adply(movies[ ,'Genres'], 1, function(x){
  dummy_genre <- rep(0, length(genres))
  g <- strsplit(x, split = '[|]')
  index_g <- aaply(g[[1]], 1, function(x){
    return(which(genres == x))
  })
  dummy_genre[index_g] <- 1
  return(dummy_genre)
})

movie_genre <- movie_genre[ ,-1]

genres <- c("Action", "Adventure", "Animation", "Childrens",
            "Comedy", "Crime", "Documentary", "Drama", "Fantasy",
            "FilmNoir", "Horror", "Musical", "Mystery", "Romance",
            "SciFi", "Thriller", "War", "Western")
colnames(movie_genre) <- paste("genres", genres, sep = "_")
movies <- cbind(movies, movie_genre)
rownames(movie_genre) <- movies$MovieID

data <- alply(genres, 1, function(genre){
                          query <- paste("SELECT r.user, avg(r.rating * m.genres_", genre,") as avg_", genre, "_rating
                                          FROM ratings as r
                                          INNER JOIN movies as m on r.movie = m.MovieID
                                          WHERE m.genres_", genre, " != 0
                                          GROUP BY r.user
                                          ORDER BY r.user", sep = "")
                          result <- sqldf(query)
                          return(result)
})

user_genre <- ldply(data, function(x){
                      unique_users <- data.frame(unique(ratings$user))
                      colnames(unique_users) <- "user"
                      query <- paste("SELECT u.user, x.*
                                     FROM unique_users as u
                                     LEFT JOIN x as x ON u.user = x.user")
                      result <- sqldf(query)
                      result[is.na(result[ ,3]), 3] <- 0
                      return(data.frame(t(result[ ,3])))
})

user_genre <- t(user_genre)
rownames(user_genre) <- unique(ratings$user)
colnames(user_genre) <- paste("avg", genres, "rating", sep = "_")
```

### Reduction of Features

- We can use Principal Component Analysis to see if we can reduce total number of features
- Need to normalize the variables before performing the PCA

```{r, echo = TRUE, warning = FALSE, message = FALSE}
ug_mean <- apply(user_genre, 2, mean)
ug_sd <- apply(user_genre, 2, sd)
for(i in 1:dim(user_genre)[2]){
  user_genre[ ,i] <- (user_genre[ ,i] - ug_mean[i])/ug_sd[i]
}

fit <- principal(user_genre, nfactors = dim(user_genre)[2], rotate="varimax")
fit
#fit$loadings
```

We can see that PCA doesn't help in reducing the number of features. Thus we shall go ahead with original features rather than principal components

### Clustering

#### Theory
- There are two main types of clustering
  + Heirerichal Clustering: 
    + There are two approaches agglomerative and divisive
    + There are many linkage criteria but most frequent ones are
      + Complete Linkage: max{d(a, b): a E A, b E B}
      + Single Linkage: min{d(a, b): a E A, b E B}
      + Mean Linkage: 1/|A||B| E E d(a, b)
      + Wards Criterion: Decrease in variance for the cluster being merged
    + Use Dendogram to visualize the clustering
  + K Means Clustering
    + Need to fix K before performing Clustering
    + Can be used as first stage of classification algorithm

- Distance metrix that is used could any of the below
  + Euclidean: Usual distance between two vectors
  + Maximum: Maximum distance between two components of x and y
  + Manhattan: Absolute distance between two vectors
  + Binary: The vectors are regarded as binary bits, so non-zero elements are 'on' and zero elements are 'off'. The distance is the proportion of bits in which only one is on amongst those in which at least one is on.
  + Other popular distances are canberra, minkowski, cosine etc.
  
#### Analysis
- We begin with Heirerichal Clustering as we don't know exact number of clusters we need.
I have used eucledian distance as measure and performed agglomerative using both Average Linkage and Wards Criteria Method.

```{r, echo = TRUE, warning = FALSE, message = FALSE}
distances <- dist(user_genre)

# Hierarchical clustering
clusterUsers_Ward <- hclust(distances, method = "ward.D") 
clusterUsers_Average <- hclust(distances, method = "average")

# Plot the dendrogram
plot(clusterUsers_Ward)
plot(clusterUsers_Average)
```

- Looking at the dendograms we can see that Wards Criteria does better clustering. Ward's method creates one larger cluster, and a handful of smaller clusters that appear to be more evenly distributed than the average linkage method. When classifying future observations into clusters it will be desirable to have larger clusters. The size of theclusters are similar to sample sizesin a linear regression model. If one cluster is very small, it will not be well represented, thereby resulting in a greater probability of misclassification duringthe classification analysis and potentially mis-repesenting the population. For this reason, it was decided to go with Ward???s method asthe grouping criteria

- There are number of ways to identify ideal number of clusters but we use
  + Cubic Cluster Criterian: Comparative measure of the deviation of the clusters from the distribution expected if data points were obtained from a uniform distribution
  
```{r, echo = TRUE, warning = FALSE, message = FALSE}
clusterUsers_Ward$call <- NULL
ccc_stat <- NbClust(user_genre, distance = "euclidean", min.nc = 2, max.nc = 20, 
        method = "ward.D", index = "ccc")
names(ccc_stat)
plot(ccc_stat$All.index)
```

**Cluster values for local peaks are 8 and 13**

  + PseudoT2: Measures the seperation between clusters that are recently joined

```{r, echo = TRUE, warning = FALSE, message = FALSE}
pseudot2_stat <- NbClust(user_genre, distance = "euclidean", min.nc = 2, max.nc = 20, 
                   method = "ward.D", index = "pseudot2")

plot(pseudot2_stat$All.index)
```

**Cluster values after immediate peak are 3, 5, 9, 12, 14 and 17**

  + Pseudo-F: The pseudo-F statistic is intended to capture the 'tightness' of clusters, and is in essence a ratio of the mean sum of squares between groups to the mean sum of squares within group
  
```{r, echo = TRUE, warning = FALSE, message = FALSE}
pseudof_stat <- NbClust(user_genre, distance = "euclidean", min.nc = 2, max.nc = 20, 
                         method = "ward.D", index = "ch")

plot(pseudof_stat$All.index)
```

**Cluster values for Relative peaks are 2, 5, 7, 11, 13 and 15**

Based upon the plots clusters 5, 9 and 14 seem to be correct clusters

- We shall use K means clustering using the above clustering and use comparitive measures to decide final clustering
  + RMS_Standard_Deviation: Root Means Square Standard Deviation within a cluster. Lesser the better
  + Nearest Cluster: Its the cluster whose centre is nearest to the given cluster
  + Centroid Distance: Its the distance for the nearest centre to the given cluster
```{r, echo = TRUE, warning = FALSE, message = FALSE}

#Find stats similar to SAS for K means
cluster_stats <- function(data, k){
  cluster <- kmeans(data, k)
  RMS_STD <- sqrt(cluster$withinss/cluster$size)
  cluster_distances <- data.frame(as.matrix(dist(cluster$centers, diag = TRUE, upper = TRUE)))
  nearest_clusters_stats <- alply(cluster_distances, 1, function(x){
    return(list(distance = min(x[x > 0]), cluster = which(x == min(x[x > 0]))))
  })
  nearest_clusters_stats <- ldply(nearest_clusters_stats, function(x){
    return(c(x$distance, x$cluster))
  })
  nearest_clusters_stats <- nearest_clusters_stats[  ,-c(1:dim(nearest_clusters_stats)[1])]
  result <- cbind(data.frame(cluster$size), cbind(data.frame(RMS_STD), nearest_clusters_stats))
  colnames(result) <- c("Frequency", "RMS_STD", "CentroidDistance", "NearestCluster")
  return(result)
}
cluster_stats(data = user_genre, k = 5)
cluster_stats(data = user_genre, k = 9)
cluster_stats(data = user_genre, k = 14)
```
- Looking at the mean value for each of the variable in 9 clusters we can name them as follows:
	1. Dont like Musical
	2. Dont like Animation and Childrens
	3. Prefer Romance Comedy over Action Thriller Crime
	4. Dont like Fantasy
	5. Neutral
	6. Thriller no Adventure
	7. Dont like anything
	8. No Documentary
	9. Documentary
	
### Conclusions
Based upon above tables we see that Cluster 14 has clusters with size very small and 1 has a cluster with very large. Also looking at nearest neighbour, RMS_STD and Centroid Distance **9 Means Clustering** seems to be the best option.