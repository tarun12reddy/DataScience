---
title: "Football Win"
author: "Tarun Reddy Aleti"
date: "March 4th, 2016"
output: html_document
---

### Executive Summary
- The data set at hand covers all the "Serie A" football matches for the last 8 years of Italian Football League.
- Our goal is to predict the final result, having at your disposal the odds of various bookmakers.
- Need to use 6 years of data and predict the result for next 2 years.
- Best model is the one which predicts accurately more often.
- The dataset for the model can be downloaded from [train](https://inclass.kaggle.com/c/football-data-challenge/download/train.csv), [test](https://inclass.kaggle.com/c/football-data-challenge/download/test.csv)

#### Required Packages

Following packages are required for the code to run efficiently.

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
library(MASS)
library(rjson)
library(plyr)
library(knitr)
library(caret)
library(fmsb)
library(e1071)
library(randomForest)
library(caretEnsemble)
library(ROCR)
library(ineq)
library(zipcode)
library(sqldf)
library(ggplot2)
library(lubridate)
library(psych)
library(caTools)
library(rpart)
library(party)
library(nnet)
```

#### Preliminary Analysis

For the code to execute, all the datasets should be in the R working directory. The datasets are in .csv format.

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
set.seed(1)
data <- read.csv("train.csv", header = TRUE, stringsAsFactors = FALSE)
final_test <- read.csv("test.csv", header = TRUE, stringsAsFactors = FALSE)
final_test$FTR <- NA
full_data <- rbind(data, final_test)
```

- The data set under consideration has a total of **`r nrow(data)`** rows and **`r ncol(data)-2`** independent variables. Our dependent variable is *FTR (Full Time Result)* and is a categorical variable, wherein **A** indicates that Away team won, **H** indicates that home team won and **D** indicates that result was a draw
- Independent Variables are as follows

	1. *Date*: Indicates when the match was played
		+ Can extract Weekday/Weekend, DayofWeek and Month information
	2. *HomeTeam*: Name of Home Team in the match
		+ There are `r length(levels(as.factor(data$HomeTeam)))` different teams. We might need to create dummy variables for them.
		+ The team names are `r unique(data$HomeTeam)`.
	3. *AwayTeam*: Name of Away Team in the match
		+ There are `r length(levels(as.factor(data$AwayTeam)))` different teams. We might need to create dummy variables for them.
		+ The team names are `r unique(data$AwayTeam)`.
		+ There are same number of Home Teams as Away Teams
	4. *B365H*: Bet365 home win odds
	5. *B365D*: Bet365 draw odds
  6. *B365A*: Bet365 away win odds
  7. *BWH*: Bet&Win home win odds
  8. *BWD*: Bet&Win draw odds
  9. *BWA*: Bet&Win away win odds
  10. *IWH*: Interwetten home win odds
  11. *IWD*: Interwetten draw odds
  12. *IWA*: Interwetten away win odds
  13. *LBH*: Ladbrokes home win odds
  14. *LBD*: Ladbrokes draw odds
  15. *LBA*: Ladbrokes away win odds
  16. *VCH*: VC Bet home win odds
  17. *VCD*: VC Bet draw odds
  18. *VCA*: VC Bet away win odds
  19. *WHH*: William Hill home win odds
  20. *WHD*: William Hill draw odds
  21. *WHA*: William Hill away win odds

### Exploratory Data Analysis

1. *Frequency of Results*:

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
ggplot(data, aes(FTR)) + geom_bar() + ggtitle('Frequency of Result')
```

Home team is more likely to win compare to Away Team or Draw Result. We can use Home Team as our Reference in Analysis

2. Frequency of Games played by Teams at Home:

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide', fig.width = 14, fig.height = 6}
ggplot(data, aes(HomeTeam, fill = FTR)) + geom_bar() + ggtitle('Frequency of Wins per Home Team')
```

Not all teams played same number of home games. They can be categorized into 4 types based upon figure above. Overall home team is likely to win across all teams. 

3. Frequency of Games played by Teams Away:

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide', fig.width = 14, fig.height = 6}
ggplot(data, aes(AwayTeam, fill = FTR)) + geom_bar() + ggtitle('Frequency of Wins per Away Team')
``` 

Not all teams played same number of away games. They can be categorized into 4 types based upon figure above. Overall home team is likely to win across all teams. One thing to notice is that number of home games played by a team are equal to number of away games played.

4. Average Ratings per Result type for Home Team

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide', fig.width = 16, fig.height = 16}
query <- paste("SELECT ", paste('avg(', colnames(data)[6:23], '),', collapse = ""), " HomeTeam, FTR ",
               "FROM data
                GROUP BY HomeTeam, FTR", sep = "")
avg_data <- sqldf(query)
colnames(avg_data) <- c(paste('avg', colnames(data)[6:23], sep = "_"), "HomeTeam", "FTR")
l_avg_data <- alply(avg_data[ ,-c(19, 20)], 2, function(x){
                                                 y <- cbind(x, avg_data[ ,c(19, 20)])
                                                 colnames(y)[1] <- "Avg_Odds"
                                                 return(y)})
n_avg_data <- ldply(l_avg_data, function(x){x})
colnames(n_avg_data)[1] <- "Bid"
n_avg_data[ ,1] <- gsub("avg_", "", n_avg_data[ ,1])
ggplot(n_avg_data, aes(x = HomeTeam, y = Avg_Odds, colour = FTR)) + facet_wrap( ~Bid, nrow = 6, ncol = 3) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
xlab('Home Teams') + ylab('Avg Odds') +
ggtitle('Avg Odds by Bidder across Home Teams')
```

Bidding average for home team win are more than draw and draw is slightly more than losing. Also some teams have high average for bid value across compared to other teams and in such cases the bid value of losing is low

5. Average Ratings per Result type for Away Team

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide', fig.width = 16, fig.height = 16}
query <- paste("SELECT ", paste('avg(', colnames(data)[6:23], '),', collapse = ""), " AwayTeam, FTR ",
               "FROM data
                GROUP BY AwayTeam, FTR", sep = "")
avg_data <- sqldf(query)
colnames(avg_data) <- c(paste('avg', colnames(data)[6:23], sep = "_"), "AwayTeam", "FTR")
l_avg_data <- alply(avg_data[ ,-c(19, 20)], 2, function(x){
                                                 y <- cbind(x, avg_data[ ,c(19, 20)])
                                                 colnames(y)[1] <- "Avg_Odds"
                                                 return(y)})
n_avg_data <- ldply(l_avg_data, function(x){x})
colnames(n_avg_data)[1] <- "Bid"
n_avg_data[ ,1] <- gsub("avg_", "", n_avg_data[ ,1])
ggplot(n_avg_data, aes(x = AwayTeam, y = Avg_Odds, colour = FTR)) + facet_wrap( ~Bid, nrow = 6, ncol = 3) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
xlab('Away Teams') + ylab('Avg Odds') +
ggtitle('Avg Odds by Bidder across Away Teams')
```

This reflects almost mirror results of the above graph away team has low bid value of winning and the teams which have high bid value of winning home have more bid value of winning away game too.

### Dummy Variables

First we need to create dummy variables for all the categorical variables like HomeTeam and AwayTeam

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
fac_var <- c("HomeTeam", "AwayTeam")

for(var in fac_var){
	full_data[ ,var] <- as.factor(full_data[ ,var])
}

dumm_var <- fac_var

for(dumm in dumm_var){
	dumm_data <- model.matrix( ~ full_data[ ,dumm] - 1, data = full_data)
	colnames(dumm_data) <- paste(dumm, gsub("full_data|, dumm|[[:punct:]]", "", colnames(dumm_data)), sep = "_")
	full_data <- cbind(full_data, dumm_data)
}

colnames(full_data) <- gsub(" ", "", colnames(full_data))
final_data <- full_data[1:dim(data)[1], ]
final_test <- full_data[(dim(data)[1]+1):(dim(data)[1]+dim(final_test)[1]), ]
```

### Missing data - Data Imputation

The bid odds have data missing with `r sum(is.na(final_data[ ,'B365H']))` for Bet365, `r sum(is.na(final_data[ ,'BWH']))` for Bet&Win, `r sum(is.na(final_data[ ,'IWH']))` for InterWetten, `r sum(is.na(final_data[ ,'LBH']))` for Ladbrokes and `r sum(is.na(final_data[ ,'WHH']))` for WilliamHill and `r sum(is.na(final_data[ ,'VCH']))` for VC Bet. We need to impute data for each of them. We simultaneously need to fill the missing data in the test set.

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
for(bet in colnames(final_data)[6:23]){
  missing_bet <- data[(is.na(final_data[ ,bet])), ]
  missing_test <- final_test[(is.na(final_test[ ,bet])), ]
  if(dim(missing_bet)[1] > 0){
    for(i in 1:dim(missing_bet)[1]){
      x <- data[data$HomeTeam == missing_bet[i, 'HomeTeam'] & data$AwayTeam == missing_bet[i, 'AwayTeam'], ]
      if(dim(x)[1] == 1){
        x <- data[(data$HomeTeam == missing_bet[i, 'HomeTeam'] | data$AwayTeam == missing_bet[i, 'AwayTeam']) & year(data$Date) == year(missing_bet[i, 'Date']), ]
      }
      x$rate <- 1
      x[x[ ,'FTR'] == missing_bet[i, 'FTR'], 'rate'] <- 2
      final_data[which(is.na(data[ ,bet]))[i], bet] <- sum(x$rate * x[ ,bet], na.rm = TRUE)/sum(x$rate, na.rm = TRUE)
    }
  }
  if(dim(missing_test)[1] > 0){
    for(i in 1:dim(missing_test)[1]){
      x <- data[data$HomeTeam == missing_test[i, 'HomeTeam'] & data$AwayTeam == missing_test[i, 'AwayTeam'], ]
      final_test[as.numeric(which(is.na(final_test[ ,bet])))[1], bet] <- mean(x[ ,bet])
    }
  }
}
```

Used other records available for same home team and away team and imputed using weighted mean of the data where data with same result is given twice the weightage compared with other data.

### Near Zero Variance
 
We need not consider independent variables which do not enough variance in itself. We can use nzv of caret package for this analysis. Following `r length(colnames(final_data)[nzv(final_data)])` variables have near zero variance and we need not consider them further: `r colnames(final_data)[nzv(final_data)]`

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
nzv_var <- nzv(final_data)
final_data <- final_data[ ,-nzv_var]
final_test <- final_test[ ,-nzv_var]
```

### Creating Train and Test Samples

- We have a total of **`r nrow(final_data)`** rows and **`r ncol(final_data)-2`** independent variables. Now that we have a clean data set before we head to modeling, we need to create a train and test data sets.

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
trainIndex <- createDataPartition(final_data$FTR, p = .7, list = FALSE)
train <- final_data[trainIndex, ]
test <- final_data[-trainIndex, ]
```

- Size of train data set is **`r nrow(train)`** and test data set is **`r nrow(test)`**

### Outlier Removal

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
train <- train[train[ ,'B365H'] < (mean(train[ ,'B365H']) + 2.58 * sd(train[ ,'B365H'])) &
               train[ ,'B365H'] > (mean(train[ ,'B365H']) - 2.58 * sd(train[ ,'B365H'])) &
               train[ ,'B365D'] < (mean(train[ ,'B365D']) + 2.58 * sd(train[ ,'B365D'])) &
               train[ ,'B365D'] > (mean(train[ ,'B365D']) - 2.58 * sd(train[ ,'B365D'])) &
               train[ ,'B365A'] < (mean(train[ ,'B365A']) + 2.58 * sd(train[ ,'B365A'])) &
               train[ ,'B365A'] > (mean(train[ ,'B365A']) - 2.58 * sd(train[ ,'B365A'])) &
               train[ ,'BWH'] < (mean(train[ ,'BWH']) + 2.58 * sd(train[ ,'BWH'])) &
               train[ ,'BWH'] > (mean(train[ ,'BWH']) - 2.58 * sd(train[ ,'BWH'])) &
               train[ ,'BWD'] < (mean(train[ ,'BWD']) + 2.58 * sd(train[ ,'BWD'])) &
               train[ ,'BWD'] > (mean(train[ ,'BWD']) - 2.58 * sd(train[ ,'BWD'])) &
               train[ ,'BWA'] < (mean(train[ ,'BWA']) + 2.58 * sd(train[ ,'BWA'])) &
               train[ ,'BWA'] > (mean(train[ ,'BWA']) - 2.58 * sd(train[ ,'BWA'])) &
               train[ ,'IWH'] < (mean(train[ ,'IWH']) + 2.58 * sd(train[ ,'IWH'])) &
               train[ ,'IWH'] > (mean(train[ ,'IWH']) - 2.58 * sd(train[ ,'IWH'])) &
               train[ ,'IWD'] < (mean(train[ ,'IWD']) + 2.58 * sd(train[ ,'IWD'])) &
               train[ ,'IWD'] > (mean(train[ ,'IWD']) - 2.58 * sd(train[ ,'IWD'])) &
               train[ ,'IWA'] < (mean(train[ ,'IWA']) + 2.58 * sd(train[ ,'IWA'])) &
               train[ ,'IWA'] > (mean(train[ ,'IWA']) - 2.58 * sd(train[ ,'IWA'])) &
               train[ ,'LBH'] < (mean(train[ ,'LBH']) + 2.58 * sd(train[ ,'LBH'])) &
               train[ ,'LBH'] > (mean(train[ ,'LBH']) - 2.58 * sd(train[ ,'LBH'])) &
               train[ ,'LBD'] < (mean(train[ ,'LBD']) + 2.58 * sd(train[ ,'LBD'])) &
               train[ ,'LBD'] > (mean(train[ ,'LBD']) - 2.58 * sd(train[ ,'LBD'])) &
               train[ ,'LBA'] < (mean(train[ ,'LBA']) + 2.58 * sd(train[ ,'LBA'])) &
               train[ ,'LBA'] > (mean(train[ ,'LBA']) - 2.58 * sd(train[ ,'LBA'])) &
               train[ ,'WHH'] < (mean(train[ ,'WHH']) + 2.58 * sd(train[ ,'WHH'])) &
               train[ ,'WHH'] > (mean(train[ ,'WHH']) - 2.58 * sd(train[ ,'WHH'])) &
               train[ ,'WHD'] < (mean(train[ ,'WHD']) + 2.58 * sd(train[ ,'WHD'])) &
               train[ ,'WHD'] > (mean(train[ ,'WHD']) - 2.58 * sd(train[ ,'WHD'])) &
               train[ ,'WHA'] < (mean(train[ ,'WHA']) + 2.58 * sd(train[ ,'WHA'])) &
               train[ ,'WHA'] > (mean(train[ ,'WHA']) - 2.58 * sd(train[ ,'WHA'])) &
               train[ ,'VCH'] < (mean(train[ ,'VCH']) + 2.58 * sd(train[ ,'VCH'])) &
               train[ ,'VCH'] > (mean(train[ ,'VCH']) - 2.58 * sd(train[ ,'VCH'])) &
               train[ ,'VCD'] < (mean(train[ ,'VCD']) + 2.58 * sd(train[ ,'VCD'])) &
               train[ ,'VCD'] > (mean(train[ ,'VCD']) - 2.58 * sd(train[ ,'VCD'])) &
               train[ ,'VCA'] < (mean(train[ ,'VCA']) + 2.58 * sd(train[ ,'VCA'])) &
               train[ ,'VCA'] > (mean(train[ ,'VCA']) - 2.58 * sd(train[ ,'VCA'])), ]
```

- For the continuous independent variables, we have to remove outliers that fall beyond 2.58 times the standard deviation on either side of the mean of that variable (2.58 signifies the two sided 99.5% confidence). 
*Note: Removed outliers after creating a train and test data sets, inorder to make sure how the performance will be in test set even with presence of outliers*
- The train data set has now **`r nrow(train)`** rows and **`r ncol(train)-1`** columns.

### Correlation

Final check we need to do is correlation among the independent continuous variables. If they have high correlation then we may use Principal Component Analysis or Factor Analysis.
There are `r sum(cor(train[ ,c(6:23)]) > 0.8 & cor(train[ ,c(6:23)]) < 1)/2` correlations that have more than 0.8 value, indicating very high correlation in the continuous independent variable. Need to normalize the data before performing factor analysis

```{r, echo = TRUE, warning = FALSE, message = FALSE}
train_mean <- apply(train[ ,c(6:23)], 2, mean)
train_sd <- apply(train[ ,c(6:23)], 2, sd)
for(i in 6:23){
  train[ ,i] <- (train[ ,i] - train_mean[i-5])/train_sd[i-5]
  test[ ,i] <- (test[ ,i] - train_mean[i-5])/train_sd[i-5]
  final_test[ ,i] <- (final_test[ ,i] - train_mean[i-5])/train_sd[i-5]
}

fit <- principal(train[ ,c(6:23)], nfactors=18, rotate="varimax")
fit$loadings
```

The first three principal components explain the variance present in all the 18 variables. First factor describes preference of home team winning against draw or losing (all the home bids have high positive value, while draw and away have low values), second factor shows home team not winning against draw or losing (home bids have negative value, while draw or away have high positive values), third factor shows preference for draw or away team winning (only draw and away have mostly positive values). We can define these components as StrongHome, StrongDrawWeakAway and WeakDrawStrongAway.

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide'}
train[ ,c('StrongHome', 'StrongDrawWeakAway', 'WeakDrawStrongAway')] <- data.matrix(train[ ,c(6:23)]) %*% data.matrix(fit$loadings[ ,c('PC1', 'PC2', 'PC3')])
test[ ,c('StrongHome', 'StrongDrawWeakAway', 'WeakDrawStrongAway')] <- data.matrix(test[ ,c(6:23)]) %*% data.matrix(fit$loadings[ ,c('PC1', 'PC2', 'PC3')])
final_test[ ,c('StrongHome', 'StrongDrawWeakAway', 'WeakDrawStrongAway')] <- data.matrix(final_test[ ,c(6:23)]) %*% data.matrix(fit$loadings[ ,c('PC1', 'PC2', 'PC3')])
```

### Models
Have attempted many type of models by changing independent variable types, tuning parameters, imputation, partition, etc. Below are the models in that category that yielded best solution.

#### Multinominal Model
```{r, echo = TRUE, warning = FALSE, message = FALSE}

logit_model <- multinom(as.formula(paste("FTR ~", paste(colnames(train)[24:54], collapse = " + "), sep = " ")), data = train, trControl = trainControl(method ="cv",number = 15))
train_logit_predict <- fitted(logit_model)
test_logit_predict <- predict(logit_model, newdata = test, "probs")
finaltest_logit_predict <- predict(logit_model, newdata = final_test, "probs")
train$pred_logit_FTR <- apply(train_logit_predict, 1, 
                              function(x){
                              return(colnames(train_logit_predict)[which(x == max(x))])
                              })
train_logit_accuracy <- sum(train$FTR == train$pred_logit_FTR)/dim(train)[1]
train_logit_accuracy 
test$pred_logit_FTR <- apply(test_logit_predict, 1,
                             function(x){
                              return(colnames(test_logit_predict)[which(x == max(x))])
                              })
test_logit_accuracy <- sum(test$FTR == test$pred_logit_FTR)/dim(test)[1]
test_logit_accuracy 
final_test$pred_logit_FTR <- apply(finaltest_logit_predict, 1,
                             function(x){
                              return(colnames(finaltest_logit_predict)[which(x == max(x))])
                              })
```

Tried changing the cross validation number from 5 to 25. After 15 fold the error didn't change. Used maximum probability approach to find the class prediction from probabilities.

#### Random Forest

```{r, echo = TRUE, warning = FALSE, message = FALSE, results = 'hide', fig.width = 12, fig.height = 12}
train$FTR <- as.factor(train$FTR)
test$FTR <- as.factor(test$FTR)
rf_model <- randomForest(FTR ~ HomeTeam + AwayTeam + StrongDrawWeakAway + StrongHome + WeakDrawStrongAway, 
                         data = train, importance=TRUE, ntree=2000, 
                         trControl = trainControl(method ="cv",number = 15))
varImpPlot(rf_model)

```

Tried changing various tuning parameters for randomForest like sample size to be selected across classes, number of trees, cross validation number, formula to train, number of variables to be selected from independent variables. But the above combination yielded best result.

```{r, echo = TRUE, warning = FALSE, message = FALSE}
train$pred_rf_FTR <- predict(rf_model, newdata = train)
test$pred_rf_FTR <- predict(rf_model, newdata = test)
final_test$pred_rf_FTR <- predict(rf_model, newdata = final_test)
table(train$FTR, train$pred_rf_FTR)
table(test$FTR, test$pred_rf_FTR)
train_rf_accuracy <- sum(train$FTR == train$pred_rf_FTR)/dim(train)[1]
train_rf_accuracy 
test_rf_accuracy <- sum(test$FTR == test$pred_rf_FTR)/dim(test)[1]
test_rf_accuracy 
```

We see that random forest fits perfectly for train data but for test data the accuracy decreases by half. Its not able to predict the class "D". Even changing the sample size doesn't help it further decreases the overall accuracy.

#### Naive Bayes

```{r, echo = TRUE, warning = FALSE, message = FALSE}
nb_model <- naiveBayes(FTR ~ HomeTeam + AwayTeam + StrongDrawWeakAway + StrongHome + WeakDrawStrongAway, 
                         data = train)
train$pred_nb_FTR <- predict(nb_model, newdata = train)
test$pred_nb_FTR <- predict(nb_model, newdata = test)
final_test$pred_nb_FTR <- predict(nb_model, newdata = final_test)
table(test$FTR, test$pred_nb_FTR)
train_nb_accuracy <- sum(train$FTR == train$pred_nb_FTR)/dim(train)[1]
train_nb_accuracy 
test_nb_accuracy <- sum(test$FTR == test$pred_nb_FTR)/dim(test)[1]
test_nb_accuracy 
```

Again model is not able to predict the class "D"

#### CART Model

```{r, echo = TRUE, warning = FALSE, message = FALSE, fig.height = 10}
ct_model <- rpart(as.formula(paste("FTR ~",paste(colnames(train)[24:54], collapse = " + "),
                                       sep = " ")),
                  method = "class", data = train)
printcp(ct_model)
plotcp(ct_model)
summary(ct_model)
plot(ct_model, uniform=TRUE, 
  	 main="Classification Tree for Football Win")
text(ct_model, use.n=TRUE, all=TRUE, cex=.8)

# prune the tree 
pct_model <- prune(ct_model, cp=ct_model$cptable[which.min(ct_model$cptable[,"xerror"]),"CP"])

# plot the pruned tree 
plot(pct_model, uniform=TRUE, 
  	main="Pruned Classification Tree for Football Win")
text(pct_model, use.n=TRUE, all=TRUE, cex=.8)

train_ct_predict <- predict(pct_model, newdata = train)
test_ct_predict <- predict(pct_model, newdata = test)
finaltest_ct_predict <- predict(pct_model, newdata = final_test)
train$pred_ct_FTR <- apply(train_ct_predict, 1, 
                              function(x){
                              return(colnames(train_ct_predict)[which(x == max(x))])
                              })
table(train$FTR, train$pred_ct_FTR)
train_ct_accuracy <- sum(train$FTR == train$pred_ct_FTR)/dim(train)[1]
train_ct_accuracy 
test$pred_ct_FTR <- apply(test_ct_predict, 1,
                             function(x){
                              return(colnames(test_ct_predict)[which(x == max(x))])
                              })
table(test$FTR, test$pred_ct_FTR)
test_ct_accuracy <- sum(test$FTR == test$pred_ct_FTR)/dim(test)[1]
test_ct_accuracy 
```

Using pruning method doesn't help much. Used same method as we used for logistic to convert probabilities to category predictions. Model still faces the bias in terms of not being able to predict the class D. Kaggle score was also pretty bad.

#### Conditional Inference Tree
```{r, echo = TRUE, warning = FALSE, message = FALSE}
cit_model <- ctree(as.formula(paste("FTR ~",paste(colnames(train)[24:54], collapse = " + "),
                                       sep = " ")), 
                   data = train)
plot(cit_model, main="Conditional Inference Tree for Football Win")

train$pred_cit_FTR <- predict(cit_model, newdata = train)
test$pred_cit_FTR <- predict(cit_model, newdata = test)
final_test$pred_cit_FTR <- predict(cit_model, newdata = final_test)
table(train$FTR, train$pred_cit_FTR)
train_cit_accuracy <- sum(train$FTR == train$pred_cit_FTR)/dim(train)[1]
train_cit_accuracy 
table(test$FTR, test$pred_cit_FTR)
test_cit_accuracy <- sum(test$FTR == test$pred_cit_FTR)/dim(test)[1]
test_cit_accuracy 
```

Performance is not much different from CART model.

#### Ensemble Approach

Tried all different models, different combinations from above in the tuneList. Most of them had high correlation with other models. Also models like NaiveBayes, CART and Conditional Inference Tree were performing poorly in predicting class D. Tried ensemble model of multinominal and random forest and it gave the best kaggle score.

```{r, echo = TRUE, warning = FALSE, message = FALSE}

en_model <- caretList(as.formula(paste("FTR ~", paste(colnames(train)[24:54], collapse = " + "),
                                       sep = " ")),
                      data = train,
                			tuneList=list(
                								rf1 = caretModelSpec(method = "rf", ntrees = 2000)),
                			trControl = trainControl(method ="cv",number = 15))

alltrain_model_preds <- lapply(en_model, predict, newdata=train, type='prob')
train_model_preds <- train_logit_predict
alltest_model_preds <- lapply(en_model, predict, newdata=test, type='prob')
test_model_preds <- test_logit_predict
allfinaltest_model_preds <- lapply(en_model, predict, newdata=final_test, type='prob')
finaltest_model_preds <- finaltest_logit_predict
for(model in rownames(summary(en_model))){
  train_model_preds <- cbind(train_model_preds, alltrain_model_preds[[model]])
  test_model_preds <- cbind(test_model_preds, alltest_model_preds[[model]])
  finaltest_model_preds <- cbind(finaltest_model_preds, allfinaltest_model_preds[[model]])
}
train$pred_en_FTR <- apply(train_model_preds, 1, 
                              function(x){
                                z <- NULL
                                j <- 0
                                total_class <- length(unique(colnames(train_model_preds)))
                                total_model <- length(rownames(summary(en_model)))
                                  for(i in 1:total_class){
                                    z <- c(z, mean(x[seq(i+j, i+(total_class * total_model)+j, total_class)]))
                                    j 
                                  }
                                return(colnames(train_model_preds)[which(z == max(z))])
                              })
test$pred_en_FTR <- apply(test_model_preds, 1, 
                              function(x){
                                z <- NULL
                                j <- 0
                                total_class <- length(unique(colnames(train_model_preds)))
                                total_model <- length(rownames(summary(en_model)))
                                  for(i in 1:total_class){
                                    z <- c(z, mean(x[seq(i+j, i+(total_class * total_model)+j, total_class)]))
                                    j 
                                  }
                                return(colnames(train_model_preds)[which(z == max(z))])
                              })
final_test$pred_en_FTR <- apply(finaltest_model_preds, 1, 
                                function(x){
                                z <- NULL
                                j <- 0
                                total_class <- length(unique(colnames(train_model_preds)))
                                total_model <- length(rownames(summary(en_model)))
                                  for(i in 1:total_class){
                                    z <- c(z, mean(x[seq(i+j, i+(total_class * total_model)+j, total_class)]))
                                    j 
                                  }
                                return(colnames(train_model_preds)[which(z == max(z))])
                              })

table(train$FTR, train$pred_en_FTR)
train_en_accuracy <- sum(train$FTR == train$pred_en_FTR)/dim(train)[1]
train_en_accuracy 
table(test$FTR, test$pred_en_FTR)
test_en_accuracy <- sum(test$FTR == test$pred_en_FTR)/dim(test)[1]
test_en_accuracy 
```

### Conclusions
1. Ensemble Model of Multinominal and Random Forest gave best kaggle score.
2. Attempted various models, tuning parameters, independent variables (weekday, month, daytype) but all of them were making the model worse.
3. One approach that could be explored is trying two differnet models one that predicts Home win or not and then if it doesn't it predicts if its Away win or Draw win. This way we may increase chances of predicting class D.